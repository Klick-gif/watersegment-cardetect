import warnings
warnings.filterwarnings('ignore')
import torch
from torch.utils.data import DataLoader
from transformers import DetrImageProcessor, DetrForObjectDetection
from transformers import get_scheduler
import torch.optim as optim
from tqdm.auto import tqdm
import os
from dataset import collate_fn, FloodDetectionDataset
from timeT import TimeTracker

time_tracker = TimeTracker()


def train_detr_model():
    # è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ç±»åˆ«ä¿¡æ¯ï¼ˆæ ¹æ®æ‚¨çš„æ•°æ®ï¼‰
    num_classes = 3  # cc, cm, lt ä¸‰ä¸ªç±»åˆ«
    
    # æ•°æ®è·¯å¾„
    yolo_data_path = "../YOLO11/yolo11n_data"
    coco_output_dir = "coco_format_data"
    
    # åŠ è½½processorï¼Œè®¾ç½®å›ºå®šçš„å›¾åƒå°ºå¯¸
    processor = DetrImageProcessor.from_pretrained("detr-r50", size={"height": 640, "width": 640})
    
    # åˆ›å»ºæ•°æ®é›†
    print("åˆ›å»ºæ•°æ®é›†...")
    train_dataset = FloodDetectionDataset(
        images_base_dir=yolo_data_path,
        annotations_file=os.path.join(coco_output_dir, "train_annotations.json"),
        processor=processor,
        split='train'
    )
    
    val_dataset = FloodDetectionDataset(
        images_base_dir=yolo_data_path,
        annotations_file=os.path.join(coco_output_dir, "val_annotations.json"),
        processor=processor,
        split='val'
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=8,  # å‡å°batch_sizeé¿å…å†…å­˜é—®é¢˜
        shuffle=True, 
        collate_fn=collate_fn,
        num_workers=2  # å…ˆè®¾ç½®ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=8,  # éªŒè¯æ—¶ä½¿ç”¨æ›´å°çš„batch_size
        shuffle=False, 
        collate_fn=collate_fn,
        num_workers=2
    )
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    print("åŠ è½½DETRæ¨¡å‹...")
    model = DetrForObjectDetection.from_pretrained(
        "detr-r50",
        num_labels=num_classes + 1,  # èƒŒæ™¯ + å®ç±»
        ignore_mismatched_sizes=True
    )
    model.to(device)
    
    param_dicts = [
        {"params": [p for n, p in model.named_parameters() if "backbone" not in n and p.requires_grad]},
        {"params": [p for n, p in model.named_parameters() if "backbone" in n and p.requires_grad], "lr": 1e-5},
    ]
    optimizer = optim.AdamW(param_dicts, lr=1e-4, weight_decay=1e-4)
    
    num_epochs = 30
    num_training_steps = num_epochs * len(train_dataloader)
    
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=num_training_steps
    )
    
    # è®­ç»ƒå¾ªç¯
    print("å¼€å§‹è®­ç»ƒ...")
    time_tracker.on_train_start()
    model.train()
    best_val_loss = float('inf')
    best_epoch = -1
    patience = 5  # æ—©åœè€å¿ƒï¼šè¿ç»­å¤šå°‘ epoch éªŒè¯é›†æ²¡æå‡å°±åœæ­¢
    no_improve_count = 0
    
    for epoch in range(num_epochs):
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        model.train()
        for batch_idx, batch in enumerate(progress_bar):
            try:
                pixel_values = batch["pixel_values"].to(device)
                pixel_mask = batch["pixel_mask"].to(device)
                labels = batch["labels"]

                # ç§»åŠ¨æ ‡ç­¾åˆ°è®¾å¤‡
                device_labels = []
                for label in labels:
                    device_label = {k: (v.to(device) if isinstance(v, torch.Tensor) else v) for k, v in label.items()}
                    device_labels.append(device_label)

                outputs = model(
                    pixel_values=pixel_values,
                    pixel_mask=pixel_mask,
                    labels=device_labels
                )
                
                loss = outputs.loss
                total_loss += loss.item()
                
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)
                optimizer.step()
                lr_scheduler.step()
                
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{total_loss/(batch_idx+1):.4f}'
                })
            except Exception as e:
                print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                continue

        torch.cuda.empty_cache()

        # éªŒè¯é›†è¯„ä¼°
        avg_train_loss = total_loss / len(train_dataloader)
        val_loss = evaluate_model(model, val_dataloader, device)

        print(f"\nEpoch {epoch+1}/{num_epochs} | è®­ç»ƒæŸå¤±: {avg_train_loss:.4f} | éªŒè¯æŸå¤±: {val_loss:.4f}")

        # åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä¼˜æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch + 1
            no_improve_count = 0

            save_path = f"detr_vehicle_flood_best"
            model.save_pretrained(save_path)
            processor.save_pretrained(save_path)
            print(f"âœ… éªŒè¯æŸå¤±æ”¹è¿›ï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹åˆ°: {save_path}")
        else:
            no_improve_count += 1
            print(f"âš ï¸ éªŒè¯æŸå¤±æœªæå‡ï¼Œè¿ç»­ {no_improve_count}/{patience} æ¬¡")

        # æ—©åœæœºåˆ¶
        if no_improve_count >= patience:
            print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼šéªŒè¯é›† {patience} æ¬¡æœªæå‡ã€‚æœ€ä½³æ¨¡å‹åœ¨ç¬¬ {best_epoch} è½® (val_loss={best_val_loss:.4f})ã€‚")
            break

    # è®­ç»ƒç»“æŸ
    print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³æ¨¡å‹å‡ºç°åœ¨ç¬¬ {best_epoch} è½®ï¼ŒéªŒè¯æŸå¤±ä¸º {best_val_loss:.4f}")
    time_tracker.on_train_end()

def evaluate_model(model, dataloader, device):
    """åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    num_batches = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="éªŒè¯"):
            try:
                pixel_values = batch["pixel_values"].to(device)
                pixel_mask = batch["pixel_mask"].to(device)
                labels = batch["labels"]
                
                # å°†labelsç§»åŠ¨åˆ°è®¾å¤‡
                device_labels = []
                for label in labels:
                    device_label = {}
                    for k, v in label.items():
                        if isinstance(v, torch.Tensor):
                            device_label[k] = v.to(device)
                        else:
                            device_label[k] = v
                    device_labels.append(device_label)
                
                outputs = model(
                    pixel_values=pixel_values,
                    pixel_mask=pixel_mask,
                    labels=device_labels
                )
                
                total_loss += outputs.loss.item()
                num_batches += 1
                
            except Exception as e:
                print(f"éªŒè¯è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                continue
    
    model.train()
    return total_loss / num_batches if num_batches > 0 else 0

if __name__ == "__main__":
    train_detr_model()